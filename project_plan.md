# Kafka Broker Implementation - Project Plan

## ğŸ¯ Project Overview

This project is a simplified implementation of an Apache Kafka broker built from scratch in JavaScript (Node.js). The goal is to understand the inner workings of Kafka by implementing its core protocol and features step by step.

### What is Apache Kafka?

Apache Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications. It's designed to handle high-throughput, fault-tolerant message streaming between different systems.

### What are we Building?

We're building a Kafka broker that can:
- Accept TCP connections from Kafka clients
- Parse and understand the Kafka wire protocol
- Respond to various Kafka API requests
- Manage topics, partitions, and messages
- Store and retrieve event data

---

## ğŸ“‹ Implementation Stages

### âœ… Stage 1: Send Response with Correlation ID

**Status:** COMPLETED

**What it does:**
- Sets up a basic TCP server listening on port 9092 (standard Kafka port)
- Accepts client connections
- Sends a hardcoded 8-byte response with correlation ID = 7

**Purpose:**
This stage introduces the basic structure of Kafka messages and responses:
- `message_size` (4 bytes): Size of header + body
- `correlation_id` (4 bytes): Unique identifier for matching requests/responses

**Key Concepts:**
- **TCP Server**: Uses Node.js `net` module to create a server
- **Binary Protocol**: Kafka uses binary data (not text/JSON)
- **Big-Endian Integers**: All integers are encoded in big-endian byte order

**Code Highlights:**
```javascript
const response = Buffer.alloc(8);
response.writeInt32BE(0, 0);  // message_size
response.writeInt32BE(7, 4);  // correlation_id
```

---

### âœ… Stage 2: Extract and Echo Correlation ID

**Status:** COMPLETED

**What it does:**
- Parses incoming request headers (Request Header v2)
- Extracts the actual correlation ID from the request
- Echoes it back in the response

**Purpose:**
Demonstrates how to parse binary Kafka protocol messages and extract specific fields.

**Request Header v2 Structure:**
```
Offset  Size  Field
------  ----  -----
0       4     message_size
4       2     request_api_key
6       2     request_api_version
8       4     correlation_id          <- We extract this!
12+     var   client_id, TAG_BUFFER
```

**Key Concepts:**
- **Binary Parsing**: Reading specific byte offsets from buffers
- **Request-Response Matching**: Correlation IDs let clients match responses to their requests
- **Protocol Fields**: Understanding the structure of Kafka headers

**Code Highlights:**
```javascript
const correlationId = data.readInt32BE(8);  // Read from offset 8
response.writeInt32BE(correlationId, 4);    // Echo back in response
```

---

### âœ… Stage 3: API Version Validation and Error Codes

**Status:** COMPLETED

**What it does:**
- Validates the `request_api_version` field from the request
- Returns error code 35 (UNSUPPORTED_VERSION) for unsupported versions
- Returns error code 0 (no error) for supported versions (0-4)

**Purpose:**
Introduces error handling and API versioning in Kafka protocol.

**Supported Versions:**
- ApiVersions API: versions 0-4

**Error Codes:**
- `0`: No error (success)
- `35`: UNSUPPORTED_VERSION

**Response Structure:**
```
Offset  Size  Field
------  ----  -----
0       4     message_size
4       4     correlation_id
8       2     error_code              <- New field!
```

**Key Concepts:**
- **API Versioning**: Each Kafka API supports multiple versions
- **Error Codes**: Standardized error codes for different failure scenarios
- **Version Negotiation**: Clients and brokers must agree on protocol versions

**Code Highlights:**
```javascript
const requestApiVersion = data.readInt16BE(6);
let errorCode = 0;
if (requestApiVersion < 0 || requestApiVersion > 4) {
  errorCode = 35; // UNSUPPORTED_VERSION
}
```

---

### âœ… Stage 4: Full ApiVersions Response

**Status:** COMPLETED

**What it does:**
- Implements complete ApiVersions v4 response body
- Returns list of supported APIs with their version ranges
- Correctly calculates and sets the `message_size` field

**Purpose:**
The ApiVersions API is crucial for Kafka clients to discover what features a broker supports.

**ApiVersions Response Body (v4) Structure:**
```
Field               Type            Size    Description
-----               ----            ----    -----------
error_code          INT16           2       Error code (0 = success)
api_keys            COMPACT_ARRAY   var     Array of supported APIs
  â”œâ”€ api_key        INT16           2       API identifier (e.g., 18 = ApiVersions)
  â”œâ”€ min_version    INT16           2       Minimum supported version
  â”œâ”€ max_version    INT16           2       Maximum supported version
  â””â”€ TAG_BUFFER     TAGGED_FIELDS   1       Optional tagged fields (empty)
throttle_time_ms    INT32           4       Throttle time in milliseconds
TAG_BUFFER          TAGGED_FIELDS   1       Optional tagged fields (empty)
```

**Currently Supported APIs:**
- **API 18 (ApiVersions)**: versions 0-4
  - Used to query broker capabilities
  - Must be supported by all Kafka brokers

**COMPACT_ARRAY Encoding:**
In Kafka's protocol, COMPACT_ARRAY uses special length encoding:
- For an array with `n` elements, the length prefix is `n + 1`
- Empty array (0 elements) is encoded as `0`
- Array with 1 element is encoded as `2` (not `1`)

**Complete Response Structure:**
```
00 00 00 13  // message_size:      19 bytes (header 4 + body 15)
XX XX XX XX  // correlation_id:    (echoed from request)
00 00        // error_code:        0 (no error)
02           // api_keys length:   2 = 1 element (COMPACT_ARRAY encoding)
00 12        // api_key:           18 (ApiVersions)
00 00        // min_version:       0
00 04        // max_version:       4
00           // TAG_BUFFER:        empty
00 00 00 00  // throttle_time_ms:  0
00           // TAG_BUFFER:        empty
```

**Key Concepts:**
- **Message Size Calculation**: `message_size` = header size + body size
- **COMPACT_ARRAY**: Special encoding format for arrays in Kafka protocol
- **TAG_BUFFER**: Extensibility mechanism for adding optional fields
- **API Discovery**: Clients use this to know what the broker supports

**Code Highlights:**
```javascript
// COMPACT_ARRAY: 1 element encoded as 2
responseBody.writeUInt8(2, offset);

// API entry
responseBody.writeInt16BE(18, offset);  // api_key: ApiVersions
responseBody.writeInt16BE(0, offset);   // min_version
responseBody.writeInt16BE(4, offset);   // max_version

// Correct message_size calculation
const messageSize = 4 + responseBody.length; // header + body
response.writeInt32BE(messageSize, 0);
```

---

### âœ… Stage 5: Advertise DescribeTopicPartitions API

**Status:** COMPLETED

**What it does:**
- Adds a second API entry to the ApiVersions response
- Advertises support for the DescribeTopicPartitions API (API key 75)
- Updates the COMPACT_ARRAY to include 2 elements instead of 1

**Purpose:**
This stage prepares the broker to handle topic metadata requests by advertising the DescribeTopicPartitions API capability.

**Supported APIs (Now Advertised):**
1. **API 18 (ApiVersions)**: versions 0-4
2. **API 75 (DescribeTopicPartitions)**: version 0

**Updated Response Structure:**
```
00 00 00 1a  // message_size:      26 bytes (header 4 + body 22)
XX XX XX XX  // correlation_id:    (echoed from request)
00 00        // error_code:        0 (no error)
03           // api_keys length:   3 = 2 elements (COMPACT_ARRAY encoding)

// First API entry
00 12        // api_key:           18 (ApiVersions)
00 00        // min_version:       0
00 04        // max_version:       4
00           // TAG_BUFFER:        empty

// Second API entry
00 4b        // api_key:           75 (DescribeTopicPartitions)
00 00        // min_version:       0
00 00        // max_version:       0
00           // TAG_BUFFER:        empty

00 00 00 00  // throttle_time_ms:  0
00           // TAG_BUFFER:        empty
```

**Key Concepts:**
- **API Discovery**: Clients use ApiVersions to discover what the broker supports
- **Multiple API Support**: Broker can advertise support for multiple APIs
- **COMPACT_ARRAY Growth**: Length prefix changes from 2 (1 element) to 3 (2 elements)
- **Version Ranges**: Different APIs can have different supported version ranges

**Code Highlights:**
```javascript
// COMPACT_ARRAY: 2 elements encoded as 3
responseBody.writeUInt8(3, offset);

// First API: ApiVersions (18)
responseBody.writeInt16BE(18, offset);  // api_key
responseBody.writeInt16BE(0, offset);   // min_version: 0
responseBody.writeInt16BE(4, offset);   // max_version: 4

// Second API: DescribeTopicPartitions (75)
responseBody.writeInt16BE(75, offset);  // api_key
responseBody.writeInt16BE(0, offset);   // min_version: 0
responseBody.writeInt16BE(0, offset);   // max_version: 0
```

**What's DescribeTopicPartitions?**
The DescribeTopicPartitions API is used to query metadata about topics and their partitions:
- Topic names and IDs
- Number of partitions per topic
- Partition leaders and replicas
- Topic configuration
- Partition states

---

### âœ… Stage 6: Implement DescribeTopicPartitions for Unknown Topics

**Status:** COMPLETED

**What it does:**
- Routes requests to appropriate API handlers based on `request_api_key`
- Parses DescribeTopicPartitions request body to extract topic names
- Responds with error code 3 (UNKNOWN_TOPIC_OR_PARTITION) for unknown topics
- Uses Response Header v1 (includes TAG_BUFFER)

**Purpose:**
This stage implements the actual request handling for the DescribeTopicPartitions API, allowing clients to query topic metadata (though all topics are currently treated as unknown).

**Request Parsing:**
The DescribeTopicPartitions request body contains:
```
Field           Type            Description
-----           ----            -----------
topics          COMPACT_ARRAY   Array of topics to query
  â””â”€ topic_name STRING          The topic name
  â””â”€ TAG_BUFFER TAGGED_FIELDS   Optional fields
TAG_BUFFER      TAGGED_FIELDS   Optional fields
```

**Response Header v1 vs v0:**
- **v0**: Only `correlation_id` (4 bytes)
- **v1**: `correlation_id` (4 bytes) + `TAG_BUFFER` (1 byte)

**DescribeTopicPartitions Response Structure:**
```
00 00 00 2f  // message_size:                 47 bytes
ab cd ef 12  // correlation_id:               (from request)
00           // TAG_BUFFER:                   empty (header v1)
00 00 00 00  // throttle_time_ms:             0
02           // topics array:                 1 element
00 03        // error_code:                   3 (UNKNOWN_TOPIC_OR_PARTITION)
04           // topic_name length:            3+1 (compact string)
66 6f 6f     // topic_name:                   "foo"
00 00 00 00  // topic_id:                     00000000-0000-0000-
00 00 00 00  //                               0000-000000000000
00 00 00 00  //                               (16 bytes, all zeros)
00 00 00 00  //
00           // is_internal:                  false
01           // partitions array:             0 elements (empty)
00 00 00 00  // topic_authorized_operations:  0
00           // TAG_BUFFER:                   empty
ff           // next_cursor:                  -1 (null)
00           // TAG_BUFFER:                   empty
```

**Key Fields Explained:**

1. **error_code = 3**: UNKNOWN_TOPIC_OR_PARTITION
   - Indicates the requested topic doesn't exist
   - Client will know to create the topic or handle the error

2. **topic_id = all zeros**: 
   - UUID format: 00000000-0000-0000-0000-000000000000
   - Represents a null/invalid topic ID

3. **is_internal = false**:
   - Kafka has internal topics (e.g., `__consumer_offsets`)
   - Our topics are not internal

4. **partitions = empty**:
   - COMPACT_ARRAY with 0 elements (encoded as 1)
   - No partition data since topic doesn't exist

5. **next_cursor = -1**:
   - Used for pagination in responses with many results
   - -1 means null (no more pages)

**Code Architecture Changes:**
- **Refactored to handler pattern**: Separate functions for each API
- `handleApiVersions()`: Handles API key 18
- `handleDescribeTopicPartitions()`: Handles API key 75
- Main connection handler routes requests based on `request_api_key`

**Parsing COMPACT_STRING:**
```javascript
// COMPACT_STRING format: length_prefix (n+1) + bytes
const nameLength = data.readUInt8(offset) - 1;
offset += 1;
const name = data.toString('utf8', offset, offset + nameLength);
```

**Parsing COMPACT_ARRAY:**
```javascript
// COMPACT_ARRAY format: length_prefix (n+1) + elements
const arrayLength = data.readUInt8(offset) - 1;
offset += 1;
// Now read 'arrayLength' elements
```

**Key Concepts:**
- **API Routing**: Different request_api_key values route to different handlers
- **Request Body Parsing**: Extracting structured data from binary protocol
- **Response Header Versioning**: Different response header versions for different APIs
- **Error Codes**: Returning appropriate errors for unknown resources
- **UUID Encoding**: 16-byte binary UUID format
- **COMPACT_STRING**: Variable-length strings with length prefix
- **NULLABLE Fields**: Using special values (-1, all zeros) to represent null

**Code Highlights:**
```javascript
// API routing based on request_api_key
if (requestApiKey === 18) {
  handleApiVersions(connection, requestApiVersion, correlationId);
} else if (requestApiKey === 75) {
  handleDescribeTopicPartitions(connection, data, correlationId);
}

// Parse COMPACT_STRING for topic name
const topicNameLength = data.readUInt8(offset) - 1;
offset += 1;
const topicName = data.toString('utf8', offset, offset + topicNameLength);

// Write UUID (all zeros for unknown topic)
responseBody.fill(0, bodyOffset, bodyOffset + 16);

// Response Header v1 includes TAG_BUFFER
response.writeUInt8(0, 8); // TAG_BUFFER after correlation_id
```

---

## ğŸ”® Future Stages (To Be Implemented)

### Stage 7: Handle Known Topics
- Store topic metadata
- Return actual topic information for known topics
- Create topics dynamically

### Stage 8: Topic Management  
- Support for creating topics
- Managing partitions
- Topic configuration

### Stage 9: Produce API
- Accept messages from producers
- Write events to partitions
- Acknowledge successful writes

### Stage 10: Fetch API
- Allow consumers to read messages
- Support offset-based reading
- Implement batching

### Stage 11: Message Storage
- Persist messages to disk
- Implement log segments
- Support for log compaction

### Stage 12: Replication (Advanced)
- Multi-broker support
- Leader election
- Partition replication

---

## ğŸ—ï¸ Architecture

### Current Implementation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Kafka Broker                      â”‚
â”‚                (Port 9092)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ TCP Connection
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                 â”‚
â”‚  Client Request (Binary)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ message_size                          â”‚     â”‚
â”‚  â”‚ request_api_key     (18 or 75)        â”‚     â”‚
â”‚  â”‚ request_api_version                   â”‚     â”‚
â”‚  â”‚ correlation_id                        â”‚     â”‚
â”‚  â”‚ client_id, TAG_BUFFER (header)        â”‚     â”‚
â”‚  â”‚ ... (request body)                    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Request Parser & Router                 â”‚
â”‚  - Extract header fields                        â”‚
â”‚  - Parse request_api_key                        â”‚
â”‚  - Route based on API key                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Key 18      â”‚   â”‚ API Key 75               â”‚
â”‚ ApiVersions     â”‚   â”‚ DescribeTopicPartitions  â”‚
â”‚ Handler         â”‚   â”‚ Handler                  â”‚
â”‚                 â”‚   â”‚                          â”‚
â”‚ - Validate ver  â”‚   â”‚ - Parse request body     â”‚
â”‚ - Return list   â”‚   â”‚ - Extract topic names    â”‚
â”‚   of supported  â”‚   â”‚ - Return topic metadata  â”‚
â”‚   APIs          â”‚   â”‚   (or error if unknown)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Broker Response (Binary)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ message_size                          â”‚     â”‚
â”‚  â”‚ correlation_id                        â”‚     â”‚
â”‚  â”‚ TAG_BUFFER (header v1 for API 75)     â”‚     â”‚
â”‚  â”‚ error_code or response data           â”‚     â”‚
â”‚  â”‚ ... (response body)                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Supported APIs:**
- **API 18 (ApiVersions)**: Returns list of supported APIs
- **API 75 (DescribeTopicPartitions)**: Returns topic metadata (currently unknown topics only)

---

## ğŸ“š Kafka Protocol Concepts

### 1. Binary Protocol
Kafka uses a custom binary protocol (not HTTP, not JSON). All data is sent as raw bytes over TCP.

**Why Binary?**
- **Performance**: Faster to parse than text formats
- **Compact**: Smaller message sizes
- **Precision**: No ambiguity in data representation

### 2. Request-Response Model
Each client request gets exactly one response from the broker.

**Flow:**
1. Client opens TCP connection to broker
2. Client sends request with unique correlation_id
3. Broker processes request
4. Broker sends response with same correlation_id
5. Client matches response to original request

### 3. Big-Endian Byte Order
All multi-byte integers use big-endian (network byte order).

**Example:**
- Integer `2` as 32-bit big-endian: `00 00 00 02`
- Integer `256` as 32-bit big-endian: `00 00 01 00`

### 4. Data Types

| Type | Size | Description |
|------|------|-------------|
| INT8 | 1 byte | Signed 8-bit integer |
| INT16 | 2 bytes | Signed 16-bit integer |
| INT32 | 4 bytes | Signed 32-bit integer |
| INT64 | 8 bytes | Signed 64-bit integer |
| STRING | Variable | Length-prefixed UTF-8 string |
| COMPACT_ARRAY | Variable | Array with special length encoding |
| TAGGED_FIELDS | Variable | Optional extensibility fields |

### 5. API Versioning
Each API (e.g., Produce, Fetch, ApiVersions) supports multiple versions:
- Versions allow protocol evolution without breaking compatibility
- Clients specify which version they want to use
- Brokers respond with same version or error if unsupported

### 6. Error Codes
Standardized error codes for different scenarios:
- `0`: No error
- `35`: UNSUPPORTED_VERSION
- `3`: UNKNOWN_TOPIC_OR_PARTITION
- `100`: UNKNOWN_TOPIC_ID
- And many more...

---

## ğŸ› ï¸ Technologies Used

- **Node.js**: Runtime environment
- **net module**: Built-in TCP networking
- **Buffer API**: Binary data manipulation
- **Big-endian encoding**: Network byte order

---

## ğŸ§ª Testing

### Manual Testing
```bash
# Start the broker
./your_program.sh

# Send a test request (in another terminal)
echo -n "0000001a0012000467890abc00096b61666b612d636c69000a6b61666b612d636c6904302e3100" | xxd -r -p | nc localhost 9092 | hexdump -C
```

### Automated Testing
The CodeCrafters platform provides automated tests that verify:
- Correct message structure
- Proper byte encoding
- Accurate field values
- Protocol compliance

---

## ğŸ“– Learning Resources

### Official Kafka Documentation
- [Kafka Protocol Guide](https://kafka.apache.org/protocol.html)
- [API Reference](https://kafka.apache.org/protocol.html#protocol_api_keys)
- [Error Codes](https://kafka.apache.org/protocol.html#protocol_error_codes)

### Key Concepts to Understand
1. **TCP/IP Networking**: How TCP connections work
2. **Binary Data**: Working with buffers and byte arrays
3. **Protocol Design**: How to design efficient binary protocols
4. **Message Brokers**: Role of message brokers in distributed systems
5. **Event Streaming**: Kafka's use cases and architecture

---

## ğŸ“ What We've Learned So Far

1. **TCP Server Implementation**
   - Creating servers with Node.js `net` module
   - Handling connections and data events

2. **Binary Protocol Parsing**
   - Reading integers from specific byte offsets
   - Understanding byte order (big-endian vs little-endian)
   - Working with Node.js Buffers
   - Parsing variable-length strings (COMPACT_STRING)
   - Parsing arrays (COMPACT_ARRAY)

3. **Kafka Wire Protocol**
   - Message structure (size, header, body)
   - Request header parsing (v0, v1, v2)
   - Response construction with different header versions
   - Request body parsing

4. **API Versioning**
   - How protocols evolve over time
   - Version negotiation between client and server
   - Error handling for unsupported versions
   - Different APIs can have different version ranges

5. **Protocol Data Types**
   - INT8, INT16, INT32 encoding
   - COMPACT_ARRAY special encoding (n+1 for n elements)
   - COMPACT_STRING encoding (length+1 prefix + bytes)
   - TAG_BUFFER for extensibility
   - UUID format (16 bytes)
   - NULLABLE types (using special values like -1)
   - BOOLEAN encoding

6. **Error Handling**
   - Returning appropriate error codes
   - Distinguishing between different error scenarios
   - Error code 3: UNKNOWN_TOPIC_OR_PARTITION
   - Error code 35: UNSUPPORTED_VERSION

7. **API Routing and Architecture**
   - Routing requests based on `request_api_key`
   - Separating handlers for different APIs
   - Code organization with handler functions
   - Different response header versions for different APIs

8. **Kafka Concepts**
   - Topics and partitions (basic understanding)
   - Topic metadata (name, ID, partitions)
   - Internal vs. external topics
   - Unknown topic handling
   - Topic UUIDs and identification

---

## ğŸš€ Next Steps

1. Implement more Kafka APIs (Produce, Fetch, etc.)
2. Add topic and partition management
3. Implement message storage (in-memory first, then persistent)
4. Add consumer group support
5. Implement log compaction
6. Add replication support

---

## ğŸ“ Notes

- This is a learning project - not production-ready
- Focuses on core protocol understanding
- Implements simplified versions of Kafka features
- Does not include clustering, replication, or persistence (yet)

---

**Last Updated:** January 8, 2026
**Current Stage:** Stage 6 - DescribeTopicPartitions Implementation for Unknown Topics Complete

